# NLP Saliency
This project evaluates biases/ inconsistencies of natural language models, namely classifiers. In particular, this project evaluates hate-speech classifiers motivated by claims of race bias when Facebook used automated hate-speech detection.


This project was extended from the machine ethics homework project from the MLSS2022 course (https://course.mlsafety.org/calendar/).


Please read the REPORT.md for the summary and discussion of the processes and findings.

## [Table of Contents](#table-of-contents)
1. [Notebooks](#notebooks)
2. [Project Organization](#project-organization)


## [Notebooks](#notebooks)
1. `01_saliency_map.ipynb` 
   - 
2. `02_further_analysis.ipynb` 
   - 
2. `03_addressing_bias_analysis.ipynb` 
   - 

## [Project Organization](#project-organization)

    ├── .gitignore                    <- files and folders to be ignored by version control system
    ├── README.md                     <- The top-level README for developers using this project.
    ├── data
    │   ├── external                  <- Data from third party sources.
    │   └── processed                 <- The final, canonical data sets for modeling.
    │
    ├── models                        <- Trained and serialized models, model predictions, or model summaries
    │
    ├── figures                       <- Generated graphics and figures to be used in reporting
    ├── *.ipynb                       <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                                    and a short `-` delimited description, e.g. `1.0-jqp-initial-data-exploration`.
    │
    ├── requirements.txt              <- The requirements file for reproducing the analysis environment, e.g.
    │                                    generated with `pip freeze > requirements.txt`
    └── utils.py                      <- Reu
--------

